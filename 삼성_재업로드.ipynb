{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNaUxWpVCwAZvX5ZrpQHNSE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tlsgptj/AIStudy/blob/main/%EC%82%BC%EC%84%B1_%EC%9E%AC%EC%97%85%EB%A1%9C%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dC_Vdrge2P_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    BlipProcessor, BlipForConditionalGeneration,\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments,\n",
        "    TextDataset, DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# 여기서 모델 변경하는거임\n",
        "blip_name = \"Salesforce/blip-image-captioning-base\"\n",
        "llm_base = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "train_csv = \"/content/samsung_data/train.csv\"\n",
        "test_csv = \"/content/samsung_data/test.csv\"\n",
        "train_img_dir = \"/content/samsung_data/train_input_images\"\n",
        "test_img_dir = \"/content/samsung_data/test_input_images\"\n",
        "finetune_txt = \"/content/finetune_data.txt\"\n",
        "finetuned_dir = \"./tiny_finetuned\"\n",
        "submission_path = \"/content/final_submit.csv\"\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# 1. 데이터 전처리\n",
        "def prepare_finetune_data():\n",
        "    processor = BlipProcessor.from_pretrained(blip_name)\n",
        "    blip_model = BlipForConditionalGeneration.from_pretrained(blip_name).to(device).eval()\n",
        "    df = pd.read_csv(train_csv)\n",
        "    samples = []\n",
        "\n",
        "    print(\"[INFO] 학습 데이터 샘플 5개 출력:\")\n",
        "    sample_count = 0\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating finetune data\"):\n",
        "        image_path = os.path.join(train_img_dir, f\"{row['ID']}.jpg\")\n",
        "        if not os.path.exists(image_path):\n",
        "            continue\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        question = row[\"Question\"]\n",
        "        choices = [row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"]]\n",
        "        answer = row[\"answer\"].strip().upper()\n",
        "        if answer not in ['A', 'B', 'C', 'D']:\n",
        "            continue\n",
        "\n",
        "        inputs = processor(images=image, text=\"Describe this image in detail.\", return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            caption_ids = blip_model.generate(\n",
        "                **inputs,\n",
        "                max_length=50,\n",
        "                num_beams=5,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=2,\n",
        "            )\n",
        "        caption = processor.tokenizer.decode(caption_ids[0], skip_special_tokens=True).strip()\n",
        "\n",
        "        prompt = (\n",
        "            f\"Image description: {caption}\\n\"\n",
        "            f\"Question: {question}\\n\"\n",
        "            f\"Choices: A. {choices[0]} B. {choices[1]} C. {choices[2]} D. {choices[3]}\\n\"\n",
        "            f\"Answer:\"\n",
        "        )\n",
        "        samples.append(f\"{prompt} {answer}\")\n",
        "\n",
        "        if sample_count < 5:\n",
        "            print(f\"\\n샘플 {sample_count+1}:\")\n",
        "            print(f\"Prompt:\\n{prompt}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            sample_count += 1\n",
        "\n",
        "    with open(finetune_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\\n\".join(samples))\n",
        "\n",
        "# 2. 모델 파인튜닝\n",
        "def finetune_llm():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(llm_base)\n",
        "    model = AutoModelForCausalLM.from_pretrained(llm_base)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    dataset = TextDataset(tokenizer=tokenizer, file_path=finetune_txt, block_size=512)\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=finetuned_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=20,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        logging_steps=50,\n",
        "        fp16=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset\n",
        "    )\n",
        "\n",
        "    print(\"[INFO] 학습 시작...\")\n",
        "    trainer.train()\n",
        "    print(\"[INFO] 학습 완료. 모델 저장 중...\")\n",
        "    model.save_pretrained(finetuned_dir)\n",
        "    tokenizer.save_pretrained(finetuned_dir)\n",
        "    print(\"[INFO] 모델 저장 완료.\")\n",
        "\n",
        "# 3. 추론\n",
        "def generate_caption(image, processor, model):\n",
        "    try:\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_length=50,\n",
        "                num_beams=5,\n",
        "                early_stopping=True\n",
        "            )\n",
        "        caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "        print(f\"[DEBUG] Generated caption: {caption}\")\n",
        "        return caption\n",
        "    except Exception as e:\n",
        "        print(f\"[Caption Error] {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def extract_choice(ans):\n",
        "    ans = ans.strip().upper()\n",
        "    for ch in ans:\n",
        "        if ch in ['A', 'B', 'C', 'D']:\n",
        "            return ch\n",
        "    return \"?\"\n",
        "\n",
        "def run_inference():\n",
        "    processor = BlipProcessor.from_pretrained(blip_name)\n",
        "    blip_model = BlipForConditionalGeneration.from_pretrained(blip_name).to(device).eval()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(finetuned_dir)\n",
        "    model = AutoModelForCausalLM.from_pretrained(finetuned_dir).to(device).eval()\n",
        "\n",
        "    df = pd.read_csv(test_csv)\n",
        "    results = []\n",
        "\n",
        "    print(\"[INFO] 추론 시작...\")\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Running inference\"):\n",
        "        try:\n",
        "            image_path = os.path.join(test_img_dir, f\"{row['ID']}.jpg\")\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            question = row[\"Question\"]\n",
        "            choices = [row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"]]\n",
        "\n",
        "            caption = generate_caption(image, processor, blip_model)\n",
        "            prompt = (\n",
        "                f\"Image description: {caption}\\n\"\n",
        "                f\"Question: {question}\\n\"\n",
        "                f\"Choices: A. {choices[0]} B. {choices[1]} C. {choices[2]} D. {choices[3]}\\n\"\n",
        "                f\"Answer:\"\n",
        "            )\n",
        "\n",
        "            print(f\"\\n[Prompt idx {idx}]\\n{prompt}\")\n",
        "\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=50,\n",
        "                    num_beams=5,\n",
        "                    early_stopping=True,\n",
        "                    no_repeat_ngram_size=2\n",
        "                )\n",
        "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "            best_choice = extract_choice(answer)\n",
        "            results.append(best_choice)\n",
        "\n",
        "            print(f\"[Output idx {idx}] Raw answer: '{answer}'\")\n",
        "            print(f\"[Output idx {idx}] 선택: {best_choice}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error at idx {idx}: {e}\")\n",
        "            results.append(\"?\")\n",
        "\n",
        "    pd.DataFrame({\"ID\": df[\"ID\"], \"answer\": results}).to_csv(submission_path, index=False)\n",
        "    print(f\"[INFO] 결과 저장 완료: {submission_path}\")\n",
        "\n",
        "# 4. 실행\n",
        "if __name__ == \"__main__\":\n",
        "    seed_everything()\n",
        "    prepare_finetune_data()\n",
        "    finetune_llm()\n",
        "    run_inference()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BlipProcessor, BlipForQuestionAnswering\n",
        "\n",
        "# 경로 설정\n",
        "test_csv = \"/content/samsung_data/test.csv\"\n",
        "test_img_dir = \"/content/samsung_data/test_input_images\"\n",
        "submission_path = \"/content/samsung_data/sample_submission.csv\"\n",
        "finetuned_dir = \"./tiny_finetuned\"  # 파인튜닝된 TinyLlama 저장 경로\n",
        "blip_name = \"Salesforce/blip-vqa-base\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 모델 로딩\n",
        "print(\"Loading models...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(finetuned_dir)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(finetuned_dir).to(device).eval()\n",
        "blip_processor = BlipProcessor.from_pretrained(blip_name)\n",
        "blip_model = BlipForQuestionAnswering.from_pretrained(blip_name).to(device).eval()\n",
        "print(\"Models loaded.\")\n",
        "\n",
        "# 이미지 캡션 생성\n",
        "def generate_caption(image):\n",
        "    try:\n",
        "        inputs = blip_processor(image, \"Describe the scene in detail.\", return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            output = blip_model.generate(**inputs)\n",
        "        return blip_processor.tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# 추론 함수\n",
        "def inference():\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    results = []\n",
        "\n",
        "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Inference\"):\n",
        "        try:\n",
        "            image_path = os.path.join(test_img_dir, f\"{row['ID']}.jpg\")\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            question = row[\"Question\"]\n",
        "            choices = [row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"]]\n",
        "\n",
        "            caption = generate_caption(image)\n",
        "            prompt = f\"Image description: {caption}\\nQuestion: {question}\\nChoices: A. {choices[0]} B. {choices[1]} C. {choices[2]} D. {choices[3]}\\nAnswer:\"\n",
        "\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = llm_model.generate(**inputs, max_new_tokens=4)\n",
        "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "            if answer and answer[0] in \"ABCD\":\n",
        "                results.append(answer[0])\n",
        "            else:\n",
        "                results.append(\"?\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            results.append(\"?\")\n",
        "\n",
        "    pd.DataFrame({\"ID\": test_df[\"ID\"], \"answer\": results}).to_csv(submission_path, index=False)\n",
        "    print(\"Saved submission to:\", submission_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()"
      ],
      "metadata": {
        "id": "c8WkHgDJe7Of"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}